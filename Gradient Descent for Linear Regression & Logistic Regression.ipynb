{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$Cost$ $Function$ $= \\frac{1}{2m} * \\sum_{i=0}^{m-1} (f_{\\_wb}X^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cost(X,y,w,b):\n",
    "    m = X.shape[0]  # size of the sample\n",
    "    cost = 0.0  # Initial cost\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        err_i = f_wb_i - y[i]\n",
    "        cost = cost + (err_i) ** 2\n",
    "        \n",
    "    cost = cost / (2 * m)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dj\\_dw$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)}) * \\overrightarrow{X}$\n",
    "\n",
    "$dj\\_db$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gradient(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i],w) + b \n",
    "        err_i = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]\n",
    "        dj_db = dj_db + err_i\n",
    "    \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = w - \\alpha * dj\\_dw$\n",
    "\n",
    "$b = b - \\alpha * dj\\_db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gradient_descent(X, y, w_inint, b_inint, alpha, num_iter, cost, gradient):\n",
    "    w = copy.deepcopy(w.inint)\n",
    "    b = b_inint\n",
    "    j_function = []\n",
    "\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        cost = cost(X, y, w, b)\n",
    "        dj_db, dj_dw = gradient(X,y,w,b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i<=10_000:\n",
    "            j_function.append(cost)\n",
    "\n",
    "        if i%(num_iter/10) == 0:\n",
    "            print(f'Iteration: {i}, cost: {j_function[-1]}, w: {w}, b: {b}')\n",
    "    return w, b, j_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Define the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_predict(X, w, b):\n",
    "    m = X.shape[0]\n",
    "    f_wb = []\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        f_wb.append(f_wb_i)\n",
    "\n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the cost function (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Cost = \\frac{1}{2m} * \\sum_{i=0}^{m-1} (f_{\\_wb}X^{(i)} - y^{(i)})^2 + \\frac{\\lambda}{2*m} * \\sum_{j=0}^{n-1} {(w^{j})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_cost_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i],w) + b\n",
    "        err_i = f_wb_i - y[i]\n",
    "        cost = cost + err_i ** 2\n",
    "\n",
    "    cost = cost/ (2 * m)\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost_i = w[j] ** 2\n",
    "        reg_cost += reg_cost_i\n",
    "    reg_cost = (lambda_ / (2 * m)) * reg_cost\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the Gradient  (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dj\\_dw$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)}) + \\frac{\\lambda}{m} * \\sum_{j=0}^{n-1}w^j$\n",
    "\n",
    "$dj\\_db$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_gradient_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        err_i = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]\n",
    "        dj_db = dj_db + err_i\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    for j in range (n):\n",
    "        dj_dw[j] = dj_dw[j] + lambda_/m * w[j]\n",
    "\n",
    "    return dj_db, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Compute the Gradient Descent  (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = w - \\alpha * dj\\_dw$\n",
    "\n",
    "$b = b - \\alpha * dj\\_db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_graient_descent_reg(X, y, w_init, b_init, alpha, num_iter, cost, gradient):\n",
    "    w = copy.deepcopy(w_init)\n",
    "    b = b_init\n",
    "    j_function = []\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        cost = cost(X, y, w, b)\n",
    "        dj_db, dj_dw = gradient(X, y, w, b)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i <= 10_000:\n",
    "            j_function.append(cost)\n",
    "        if i%(num_iter/10) == 0:\n",
    "            print(f'Iteration: {i}, cost: {j_function[-1]}, w: {w}, b:{b}')\n",
    "\n",
    "    return w, b, j_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Define the predict function  (with Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_predict_reg(X, w, b):\n",
    "    m = X.shape[0]\n",
    "    f_wb = []\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        f_wb.append(f_wb_i)\n",
    "    \n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ z = \\overrightarrow{X}* \\overrightarrow{W} + b $\n",
    "\n",
    "$ fun\\_sigmoid = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "$ cost = \\frac{1}{2 * m} * \\sum_{i=0}^{m-1}-y^{i} * np.log(fun\\_sigmoid) - (1 - y^{i}) * np.log(1 - fun\\_sigmoid)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost_i = -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)\n",
    "        cost = cost + cost_i\n",
    "    \n",
    "    cost = cost / m\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dj\\_dw$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)}) $\n",
    "\n",
    "$dj\\_db$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        err_i = sigmoid(z_i) - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]\n",
    "        dj_db = dj_db + err_i\n",
    "\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = w - \\alpha * dj\\_dw$\n",
    "\n",
    "$b = b - \\alpha * dj\\_db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_descent(X, y, w_init, b_init, alpha, num_iter, cost, gradient):\n",
    "    w = copy.deecopy(w_init)\n",
    "    b = b_init\n",
    "    j_function = []\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        cost = cost(X, y, w, b)\n",
    "        dj_db, dj_dw = gradient(X, y, w, b)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i <= 10_000:\n",
    "            j_function.appned(cost)\n",
    "        if i%(num_iter/10) == 0:\n",
    "            print(f'Iteration: {i}, cost: {j_function[-1]}, w: {w}, b: {b}')\n",
    "\n",
    "    return w, b, dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Define the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(X, w, b):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    f_wb = []\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        f_wb.append(f_wb_i)\n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the cost function (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $ z = \\overrightarrow{X}* \\overrightarrow{W} + b $\n",
    "\n",
    "$ fun\\_sigmoid = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "$ cost = \\frac{1}{2 * m} * \\sum_{i=0}^{m-1}-y^{i} * np.log(fun\\_sigmoid) - (1 - y^{i}) * np.log(1 - fun\\_sigmoid) + \\frac{\\lambda}{2*m} * \\sum_{j=0}^{n-1} {(w^{j})^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "\n",
    "    cost = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "\n",
    "        cost_i = -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)\n",
    "        cost = cost + cost_i\n",
    "    \n",
    "    cost = cost / (2 * m)\n",
    "\n",
    "    reg_cost=0\n",
    "    for j in range(n):\n",
    "        reg_cost_i = w[j] ** 2\n",
    "        reg_cost = reg_cost + reg_cost_i\n",
    "    reg_cost = lambda_ / (2 * m) * reg_cost\n",
    "\n",
    "    total_cost = cost + reg_cost\n",
    "\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the Gradient  (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$dj\\_dw$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)}) + \\frac{\\lambda}{m} * \\sum_{j=0}^{n-1}w^j$\n",
    "\n",
    "$dj\\_db$ $= \\frac{1}{m} * \\sum_{i=0}^{m-1}(f_{wb} * X^{(i)} - y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        err_i = f_wb_i - y[i]\n",
    "\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]\n",
    "        dj_db = dj_db + err_i\n",
    "    \n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + lambda_ / m * w[j]\n",
    "\n",
    "    return dj_db, dj_dw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Compute the Gradient Descent  (with Regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w = w - \\alpha * dj\\_dw$\n",
    "\n",
    "$b = b - \\alpha * dj\\_db$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient_descent(X, y, w_init, b_init, alpha, num_iter, cost, gradient):\n",
    "    w = copy.deepcopy(w_init)\n",
    "    b = b_init\n",
    "    j_function = []\n",
    "    lambda_ = 1\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        cost = cost(X, y, w, b, lambda_)\n",
    "        dj_db, dj_dw = gradient(X, y, w, b, lambda_)\n",
    "\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "        if i <=10_000:\n",
    "            j_function.appned(cost)\n",
    "\n",
    "        if i%(num_iter/10) == 0:\n",
    "            print(f'Iteration: {i}, cost: {j_function[-1]}, w: {w}, b: {b}')\n",
    "    \n",
    "    return w, b, j_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression - Define the predict function  (with Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict_reg(X, w, b):\n",
    "    m = X.shape[0]\n",
    "    f_wb = []\n",
    "\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        f_wb.append(f_wb_i)\n",
    "\n",
    "    return f_wb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
